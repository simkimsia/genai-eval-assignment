<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="LLM Evaluation Framework Presentation">
    <meta name="keywords" content="LLM, evaluation, framework, cursor">
    <title>LLM Evaluation Framework</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css">
    <style>
        .reveal h1 { font-size: 2.5em; }
        .reveal h2 { font-size: 1.8em; }
        .reveal p { font-size: 1.2em; }
        .reveal ul { font-size: 1.1em; }
        .metric { color: #2c3e50; }
        .highlight { color: #e74c3c; }
        .automated { color: #27ae60; }
        .manual { color: #2980b9; }
        .submetric { font-size: 0.9em; margin-left: 20px; }
        .results-container { display: flex; justify-content: space-around; }
        .cursor-interface { text-align: center; }
        .caption { font-size: 0.9em; color: #666; margin-top: 10px; }
        .cursor-interface img { max-height: 60vh; margin: 0 auto; display: block; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Slide 1: Title -->
            <section>
                <h1>Evaluating LLMs in Real-World Development</h1>
                <p>Beyond Traditional Benchmarks</p>
                <p><small>Claude 3.7 vs Gemini Pro 2.5 in Cursor IDE</small></p>
            </section>

            <!-- Slide 2: Task & Context -->
            <section>
                <h2>Task & Context</h2>
                <ul>
                    <li>Feature development in existing Django codebase</li>
                    <li>Testing in Cursor IDE (real-world environment)</li>
                    <li>Focus on contextual understanding</li>
                    <li class="highlight">Moving beyond i.i.d. assumptions</li>
                </ul>
            </section>

            <!-- Slide 3: Cursor IDE Interface -->
            <section>
                <h2>Cursor IDE Interface</h2>
                <div class="cursor-interface">
                    <img src="cursor-looks-like.png" alt="Cursor IDE Interface showing LLM interaction" height="400" width="600">
                    <p class="caption">Real-world development environment in Cursor IDE with Gemini Pro</p>
                </div>
            </section>

            <!-- Slide 4: Evaluation Framework -->
            <section>
                <h2>5 Key Metrics</h2>
                <ul>
                    <li class="automated">Python Code Quality Delta (30%)</li>
                    <li class="automated">Template Quality Delta (20%)</li>
                    <li class="manual">Number of Prompts (15%)</li>
                    <li class="manual">Explanation Clarity (15%)</li>
                    <li class="manual">Task Efficiency (20%)</li>
                </ul>
            </section>

            <!-- Slide 5: Automated Metrics -->
            <section>
                <h2>Automated Measurements</h2>
                <ul>
                    <li class="automated">Python Code Quality Delta
                        <ul class="submetric">
                            <li>Test coverage delta (after - before)</li>
                            <li>Pylint score delta (after - before)</li>
                            <li>New failed tests count</li>
                            <li>Measured by: step_01_run_tests.py, step_02_check_python_code_quality.py</li>
                        </ul>
                    </li>
                    <li class="automated">Template Quality Delta
                        <ul class="submetric">
                            <li>Total issues delta (after - before)</li>
                            <li>Categories: accessibility, inline styles, template tags, JavaScript, structure</li>
                            <li>Measured by: step_03_check_template_quality.py</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- Slide 6: Manual Metrics -->
            <section>
                <h2>Manual & Subjective Metrics</h2>
                <ul>
                    <li class="manual">Number of Prompts (15%)
                        <ul class="submetric">
                            <li>Count of prompts needed to complete task</li>
                            <li>Lower is better</li>
                        </ul>
                    </li>
                    <li class="manual">Explanation Clarity (15%)
                        <ul class="submetric">
                            <li>LLM-as-judge evaluation (1-5 scale)</li>
                            <li>Automated but subjective</li>
                        </ul>
                    </li>
                    <li class="manual">Task Efficiency (20%)
                        <ul class="submetric">
                            <li>Field efficiency ratio (actual/minimum)</li>
                            <li>Number of form submissions</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- Slide 7: Results -->
            <section>
                <h2>Evaluation Results</h2>
                <div class="results-container">
                    <div>
                        <h3>Claude 3.7 Sonnet</h3>
                        <ul class="submetric">
                            <li>Python Quality: 0/10</li>
                            <li>Template Quality: 8.4/10</li>
                            <li>Prompt Efficiency: 9.5/10</li>
                            <li>Explanation: 8/10</li>
                            <li>Task Efficiency: 8/10</li>
                            <li class="highlight">Final: 6.53/10</li>
                        </ul>
                    </div>
                    <div>
                        <h3>Gemini 2.5 Pro</h3>
                        <ul class="submetric">
                            <li>Python Quality: -2.54/10</li>
                            <li>Template Quality: 8.8/10</li>
                            <li>Prompt Efficiency: 9.5/10</li>
                            <li>Explanation: 8/10</li>
                            <li>Task Efficiency: 9/10</li>
                            <li class="highlight">Final: 6.12/10</li>
                        </ul>
                    </div>
                </div>
            </section>

            <!-- Slide 8: Key Insights -->
            <section>
                <h2>Key Insights</h2>
                <ul>
                    <li>Context matters more than isolated tasks</li>
                    <li>Quality deltas better reflect real impact</li>
                    <li>Efficiency metrics capture developer experience</li>
                    <li class="highlight">Real-world tasks are not i.i.d.</li>
                </ul>
            </section>

            <!-- Slide 9: Thank You -->
            <section>
                <h2>Thank You!</h2>
                <p>Questions?</p>
                <p><small>Inspired by "The Second Half of AI" by Shunyu Yao</small></p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            plugins: [ RevealHighlight ],
            transition: 'slide',
            autoPlayMedia: true
        });
    </script>
</body>
</html>
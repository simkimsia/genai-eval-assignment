<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="LLM Evaluation Framework Presentation">
    <meta name="keywords" content="LLM, evaluation, framework, cursor">
    <title>LLM Evaluation Framework</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/white.css">
    <style>
        .reveal h1 { font-size: 2.5em; }
        .reveal h2 { font-size: 1.8em; }
        .reveal p { font-size: 1.2em; }
        .reveal ul { font-size: 1.1em; }
        .metric { color: #2c3e50; }
        .highlight { color: #e74c3c; }
        .automated { color: #27ae60; }
        .manual { color: #2980b9; }
        .submetric { font-size: 0.9em; margin-left: 20px; }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            <!-- Slide 1: Title -->
            <section>
                <h1>Evaluating LLMs in Real-World Development</h1>
                <p>Beyond Traditional Benchmarks</p>
                <p><small>Claude 3.7 vs Gemini Pro 2.5 in Cursor IDE</small></p>
            </section>

            <!-- Slide 2: Task & Context -->
            <section>
                <h2>Task & Context</h2>
                <ul>
                    <li>Feature development in existing Django codebase</li>
                    <li>Testing in Cursor IDE (real-world environment)</li>
                    <li>Focus on contextual understanding</li>
                    <li class="highlight">Moving beyond i.i.d. assumptions</li>
                </ul>
            </section>

            <!-- Slide 3: Evaluation Framework -->
            <section>
                <h2>5 Key Metrics</h2>
                <ul>
                    <li class="automated">Python Code Quality Delta (30%)</li>
                    <li class="automated">Template Quality Delta (20%)</li>
                    <li class="manual">Number of Prompts (15%)</li>
                    <li class="manual">Explanation Clarity (15%)</li>
                    <li class="manual">Task Efficiency (20%)</li>
                </ul>
            </section>

            <!-- Slide 4: Automated Metrics -->
            <section>
                <h2>Automated Measurements</h2>
                <ul>
                    <li class="automated">Python Code Quality Delta
                        <ul class="submetric">
                            <li>Test coverage delta (after - before)</li>
                            <li>Pylint score delta (after - before)</li>
                            <li>New failed tests count</li>
                            <li>Measured by: step_01_run_tests.py, step_02_check_python_code_quality.py</li>
                        </ul>
                    </li>
                    <li class="automated">Template Quality Delta
                        <ul class="submetric">
                            <li>Total issues delta (after - before)</li>
                            <li>Categories: accessibility, inline styles, template tags, JavaScript, structure</li>
                            <li>Measured by: step_03_check_template_quality.py</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- Slide 5: Manual Metrics -->
            <section>
                <h2>Manual & Subjective Metrics</h2>
                <ul>
                    <li class="manual">Number of Prompts (15%)
                        <ul class="submetric">
                            <li>Count of prompts needed to complete task</li>
                            <li>Lower is better</li>
                        </ul>
                    </li>
                    <li class="manual">Explanation Clarity (15%)
                        <ul class="submetric">
                            <li>LLM-as-judge evaluation (1-5 scale)</li>
                            <li>Automated but subjective</li>
                        </ul>
                    </li>
                    <li class="manual">Task Efficiency (20%)
                        <ul class="submetric">
                            <li>Field efficiency ratio (actual/minimum)</li>
                            <li>Number of form submissions</li>
                        </ul>
                    </li>
                </ul>
            </section>

            <!-- Slide 6: Key Insights -->
            <section>
                <h2>Key Insights</h2>
                <ul>
                    <li>Context matters more than isolated tasks</li>
                    <li>Quality deltas better reflect real impact</li>
                    <li>Efficiency metrics capture developer experience</li>
                    <li class="highlight">Real-world tasks are not i.i.d.</li>
                </ul>
            </section>

            <!-- Slide 7: Thank You -->
            <section>
                <h2>Thank You!</h2>
                <p>Questions?</p>
                <p><small>Inspired by "The Second Half of AI" by Shunyu Yao</small></p>
            </section>
        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/plugin/highlight/highlight.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            plugins: [ RevealHighlight ],
            transition: 'slide',
            autoPlayMedia: true
        });
    </script>
</body>
</html>